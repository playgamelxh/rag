## BERT
BERT（Bidirectional Encoder Representations from Transformers）是 2018 年由 Google 团队提出的预训练语言模型，其核心突破是通过双向 Transformer 编码器和创新的预训练任务，让模型能真正捕捉文本的上下文语义信息，彻底改变了自然语言处理（NLP）的范式。以下从核心思想、模型结构、输入表示、预训练任务、微调流程、优势与局限六个方面详细解析。

### 一、核心思想：双向上下文建模
在 BERT 之前，主流预训练模型（如 ELMo、GPT）存在明显局限：
- ELMo：基于双向 LSTM，但属于 “浅层双向”（分别训练左→右和右→左两个单向 LSTM，再拼接结果），无法同时利用左右上下文。
- GPT：基于 Transformer 解码器，采用 “自回归” 方式（左→右单向建模），只能关注前文，无法利用后文信息。

BERT 的核心创新是：使用 Transformer 编码器的 “双向自注意力机制”，让每个 token 在编码时能同时看到左侧和右侧的所有 token，实现 “深层双向上下文建模”。例如，对于句子 “他打开了 [MASK]，取出了书”，BERT 能结合 “打开了” 和 “取出了书” 推断 [MASK] 应为 “书包” 或 “抽屉”，而单向模型只能基于 “打开了” 做片面推测。

### 二、模型结构：基于 Transformer 编码器
BERT 的主体结构完全复用了 Transformer 的编码器模块（未使用解码器，因解码器的自注意力是单向的）。其结构可概括为：
```azure
输入序列 → 嵌入层（Token+Segment+Position） → N个Transformer编码器堆叠 → 输出每个token的上下文嵌入
```
1. Transformer 编码器细节

   每个 Transformer 编码器由两个核心子层组成（与原始 Transformer 一致）：
    - 多头自注意力层（Multi-Head Self-Attention）：
      让每个 token 对序列中所有其他 token 计算 “注意力权重”，捕捉 token 间的语义关联（如 “苹果” 在 “吃苹果” 中与 “吃” 强相关）。
      具体过程：将输入嵌入拆分为多个 “头”（head）并行计算，每个头学习不同的注意力模式，最后拼接结果。
    - 前馈神经网络层（Feed-Forward Network, FFN）：
      对每个 token 的注意力输出进行非线性转换（通过两层线性变换 + ReLU 激活），增强模型拟合能力。
   
    此外，每个子层都包含残差连接（Residual Connection）和层归一化（Layer Normalization），公式为：
   Output=LayerNorm(Input+Sublayer(Input))，这两个机制能缓解深层网络的梯度消失问题，加速训练。
2. 模型规模

    BERT 提供两个基础版本，差异在于网络深度（层数）和宽度（隐藏层维度）：
    - BERT-Base：12 层 Transformer 编码器，隐藏层维度 768，12 个注意力头，总参数量约 110M。
    - BERT-Large：24 层 Transformer 编码器，隐藏层维度 1024，16 个注意力头，总参数量约 340M。
      层数越多、隐藏维度越大，模型拟合能力越强，但训练成本也越高。

### 三、输入表示：统一格式适配多任务
BERT 设计了灵活的输入表示方式，能处理单句、句子对（如问答、句间关系判断）等多种输入形式，且输出长度固定（与输入 token 数量一致）。输入表示由三部分叠加而成：
1. Token 嵌入（Token Embeddings）
    - 分词方式：采用WordPiece分词（而非传统空格分词），将词拆分为子词单元（Subword），平衡词汇量和未登录词（OOV）问题。
      例：“unhappiness” → 拆分为 “un”、“happiness”；“苹果手机” → 拆分为 “苹果”、“手机”（中文通常先分字再聚合）。
    - 特殊 Token：
        + [CLS]：放在输入序列的首位，用于分类任务（如情感分析），其输出被视为整个序列的 “聚合表征”。
        + [SEP]：作为句子分隔符，用于句子对任务（如 “前提→假设”），例：[CLS] 句子A [SEP] 句子B [SEP]。
        + [PAD]：用于填充短序列，使批量输入长度一致，填充部分在注意力计算中会被忽略。
2. 段嵌入（Segment Embeddings）

    用于区分句子对中的两个句子（如判断 A 和 B 是否为连续句子）。
    - 句子 A 中的所有 token 分配 “段 ID 0”，句子 B 中的所有 token 分配 “段 ID 1”，通过嵌入向量区分。
    - 单句任务中，所有 token 的段 ID 均为 0。
3. 位置嵌入（Position Embeddings）

    Transformer 本身是 “无位置感知” 的（对 token 顺序不敏感），因此需要显式注入位置信息。   
    - BERT 的位置嵌入是可学习的（与原始 Transformer 的正弦余弦固定位置编码不同），每个位置（从 0 到 511）对应一个唯一的嵌入向量。
    - 最大支持长度为 512 个 token（超过则截断）。
    
   输入表示示例：
   对于句子对 “我爱自然语言处理。它很有趣。”，输入序列及嵌入如下： 
```azure
[CLS] 我 爱 自 然 语 言 处 理 。 [SEP] 它 很 有 趣 。 [SEP] [PAD] ...  
↑      ↑↑↑↑↑↑↑↑↑↑↑↑   ↑      ↑↑↑↑↑↑↑↑↑↑   ↑  
CLS   句子A的Token   SEP   句子B的Token  SEP  PAD  

最终输入嵌入 = Token嵌入 + 段嵌入（句子A为0，句子B为1） + 位置嵌入（0,1,2,...）
```

### 四、预训练任务：让模型 “学语言”
BERT 通过无监督预训练（在大规模文本语料上训练）学习通用语言知识，核心是两个创新任务：掩码语言模型（MLM） 和下一句预测（NSP）。
1. 掩码语言模型（Masked Language Model, MLM）

    目标：让模型根据上下文预测被 “掩盖” 的 token，强制学习双向上下文依赖。
    - 过程：
      1. 随机选择输入序列中 15% 的 token 进行处理；
      2. 对选中的 token：
          + 80% 概率替换为[MASK]（如 “我爱 [MASK]” → 预测 “中国”）；
          + 10% 概率替换为随机 token（如 “我爱 [MASK]”→“我爱桌子”，强迫模型关注上下文而非记住 token）；
          + 10% 概率保留原 token（如 “我爱中国”→ 仍为 “我爱中国”，减少训练与推理的差异）；
      3. 模型通过输出层（线性变换 + Softmax）预测被处理的 token，损失为预测误差。
    - 优势：与传统自回归模型（如 GPT）的 “左→右预测” 不同，MLM 允许模型同时利用左右上下文，实现真正的双向学习。
2. 下一句预测（Next Sentence Prediction, NSP）

   目标：让模型学习句子间的逻辑关系（如是否连续），辅助理解篇章结构。
    - 过程：
      1. 从语料中抽取句子对（A, B）；
      2. 50% 概率 B 是 A 的真实下一句（正例），50% 概率 B 是随机句子（负例）；
      3. 模型通过[CLS] token 的输出预测 “B 是否为 A 的下一句”（二分类），损失为分类误差。
    - 作用：帮助模型捕捉句子级语义（如 “他买了菜” 的下一句更可能是 “然后做了饭” 而非 “太阳从西边升起”）。
      
预训练语料

BERT 在两个大规模无标注语料上预训练：
- BookCorpus：包含约 8 亿词的书籍文本（以叙事性文本为主）；
- English Wikipedia：约 25 亿词的百科文本（以事实性文本为主）。

大规模、多样化的语料确保模型学习到通用语言规律。

### 五、微调流程：适配下游任务
预训练完成后，BERT 通过 “微调”（Fine-tuning）适配具体下游任务。核心是：冻结预训练参数（或微调），仅修改输出层，用任务数据训练少量参数。

不同任务的微调方式如下：
1. 文本分类（如情感分析、主题分类）
    - 输入：单句或篇章（以[CLS]开头）；
    - 输出：取[CLS]的嵌入向量，接一个线性层 + Softmax，预测类别（如 “正面 / 负面”）。
2. 句子对分类（如自然语言推理、语义相似度）
    - 输入：句子对（A + [SEP] + B + [SEP]）；
    - 输出：取[CLS]的嵌入向量，接分类层预测关系（如 “蕴含 / 矛盾 / 中立”）。
3. 问答（如 SQuAD 任务）
    - 输入：问题 + [SEP] + 篇章（包含答案）；
    - 输出：预测答案在篇章中的 “起始位置” 和 “结束位置”：
        + 对每个 token 的嵌入接线性层，计算 “是起始位置” 的概率；
        + 同理计算 “是结束位置” 的概率；
        + 损失为起始和结束位置的预测误差。
4. 命名实体识别（NER）
    - 输入：单句；
    - 输出：对每个 token 的嵌入接分类层，预测实体类型（如 “人名 / 地名 / 机构名”）。

通过这种 “预训练 + 微调” 模式，BERT 避免了每个任务从头训练模型，显著提升了小数据场景下的性能。

### 六、优势与局限
优势：
    1. 双向上下文理解：Transformer 编码器的自注意力机制让每个 token 能同时利用左右上下文，远超单向模型。
    2. 强泛化能力：大规模预训练学到的语言知识可迁移到几乎所有 NLP 任务，尤其在小数据集上表现优异。
    3. 统一框架：通过灵活的输入输出设计，用同一模型架构解决分类、问答、NER 等多任务，简化了 NLP 系统设计。

局限：
    1. 长文本处理能力有限：最大输入长度为 512token（约 300-400 个中文词），无法直接处理长文档（如论文、书籍）。
    2. 预训练成本高：BERT-Large 训练需数千 GPU 小时，普通研究者难以复现。
    3. 静态掩码偏差：MLM 在预训练时仅掩码 15% 的 token，且掩码位置固定，可能导致模型对未掩码 token 的学习不足。
    4. 推理速度较慢：Transformer 的自注意力计算复杂度为 O (n²)（n 为序列长度），长文本推理效率低。

### 七、影响与后续发展
BERT 的提出标志着 NLP 进入 “预训练时代”，后续一系列模型（如 RoBERTa、ALBERT、SpanBERT）均基于 BERT 改进：
- RoBERTa：移除 NSP 任务，用动态掩码和更长序列训练，性能超越 BERT；
- ALBERT：通过参数共享和因式分解嵌入维度，大幅减少参数量；
- SpanBERT：掩码连续片段而非单个 token，增强对短语级语义的理解。

### 总结
BERT 通过 “双向 Transformer+MLM 预训练” 的创新，首次实现了深层上下文语义建模，彻底改变了 NLP 任务的解决方式。其 “预训练 + 微调” 范式成为现代 NLP 的标准流程，至今仍是许多工业级系统的基础模型。理解 BERT 的核心设计（双向性、掩码训练、灵活输入），是掌握后续预训练模型的关键。
